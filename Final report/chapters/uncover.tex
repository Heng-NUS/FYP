\label{sec:uncoverig}
There are massive topics related to healthcare and diseases and them expand everyday, it's impractical to list them all. \cite{tuarob2013discovering} defined health-related messages as follows: (1) either a message indicates explicitly the sick (or health problems) of the author; (2) or the message contains the author's worries about health probelms (e.g. someone else falling ill, disease outbreak). To surveille diseases, we want to cluster messages(tweets) into different groups, find out what topics are concerned about. The aim of this section is to build a model (or a ensemble of different models) that can: (1) check wether a input document is related to health; (2) assign the checked document to one best match topic. If there is no topic match it, create a new one and assign the document to it (one similar example is hot event discovery). To achieve this, the model should be able to: (1) update the parameters over new input; (2) handle unseen input (such as new words, new topics, new meaning of a seen word, etc.). As far as we know, there is no such model.
\\\\
In the existing works, there are two types of models: supervised and unsupervised. \cite{serban2019real,aramaki2011twitter,lampos2010flu,chen2017disease} use supervised machine learning and deep learning techniques to detect flu. Their experimental results show that when used to detect the known disease, supervised models can get high accuracy. However, the performance of such model highly depends on the training set. For topics that are not in the training set, supervised models cannot recognize it. 
Hence a single end-to-end supervised model (label the data in advance) is not be qualified under this scenario. \cite{paul2012model} introduces Ailment Topic Aspect Model (ATAM) that amis to classify aliments. ATAM is an unsupervised model, requires a set of aliments and the prior distribution of them. Its essence is an variant of LDA, and the hyper-parameter K (the number of diseases/topics) is defined before training. Since the aliments and symptoms are pre-defined, ATAM cannot be treated as a pure unsupervised model, and it can't recognize new health event. However, pure unsupervised models cluster data based on its structure, and therefore are not able to identify whether a document is health-related or not. In addition, the generated topics are hard to interpret. Hence, to achieve the requirements of our model, both supervised and unsupervised methods are required.
\\\\
To extract health-related tweets and topics, \cite{paul2014discovering, paul2011you} described a general framework with 2 main phases: data filtering and topic modeling. The first phase is vital for narrowing down search space. They adopt keyword searching and machine learning as data filtering method. 269 keywords and 20,000 keyphrases were collected and used to filter out irrelevant data roughly. Then 5128 tweets selected randomly from dataset were labeled for training a SVM classifier that identifies wether a input is or isn't health-related. In the second phase, probabilistic topic modeling such as LDA and ATAM are used to cluster tweets and get interpretable topics. \cite{serban2019real,sadilek2012modeling} followed the same framework to classify tweets in their system, with some variance on data labeling, keywords list and classifier. \cite{elkin2017network} used exclusion list rather a classifier in phase one. 
\\\\
Before adopting any existing framework, we used Online Biterm Term Modeling (OBTM, a unsupervised Topic modeling algorithm, details can be found in section \ref{sec:topic modeling}) on a sample of processed data, to see how many health-related topics can be found. The sample covers all processed tweets created at Jan 2018 (13,643,710 non-retweet english tweets). The algorithm took three days on calculation to cluster documents into 100 topics. Merely few topics contain words related to health while they are hard to interpret. Similar experimental results can be found in \cite{zhao2011comparing}, where the authors calculate the distribution of topic categories, and find that roughly 5\% tweets are about health. To narrow down the search space, decrease the running time and make the final results more interpretable, our project adopts a similar framework with \cite{paul2014discovering}: use the keywords search first, then cluster topics. The details and experimental results are in following sections.

\section{Keywords search}
\label{sec:Keywords search}
Influenza is one of the most common diseases and is analyzed most by researchers. For scientific comparison, it was used to test the dataset. \cite{aramaki2011twitter} used a simple word look-up of ``influenza'', which may lose massive valuable data. In inspired by \cite{lamb2013separating,lampos2010flu}, we create a list with 26 words highly related to flu based on Flu Symptoms \cite{cdc.symp} Cambridge Dictionary \cite{cambridge} and relatedwords.org \cite{relatedwords.org}. The complete word list can be seen in table \ref{tab:words list}, note that not all the words from the sources are added to our list. Professional terms are excluded since they are hardly used in colloquialism. Words that are wildly used in other scenarios (such as chill, cold) and phrases that contain keywords in our list (such as asian influenza) are not added. Then tweets are filtered according to the list (ignore case). In this step, we initially adopted a relatively lose filtering strategy: accept tweets containing any string in our list. 
\begin{table}[!htbp]
    \centering
    \hspace{0.5cm}
    \begin{tabular}{p{90pt}p{320pt}}
        Source & Word list \\ \hline
        \href{https://www.cdc.gov/flu/symptoms/symptoms.htm}{CDC} &  fever, feverish, sore throat, runny nose, stuffy nose, headache, nasal congestion, diarrhea, bluish lips, bluish face, dehydration\\ \hline
        \href{https://dictionary.cambridge.org/us/topics/disease-and-illness/colds-and-flu/}{Dictionary} & flu, catarrh, cough, common cold, influenza, sniffle, snuffle\\ \hline
        \href{https://relatedwords.org/relatedto/flu}{relatedwords.org} & h1n1, h5n1, coughing, cholera, ebola, epidemic, feverous, measles \\ \hline
    \end{tabular}
    \caption{Inclusion list}
    \label{tab:words list}
\end{table}
\\
Table \ref{tab:filtering} shows the number of left tweets after keywords filtering with this list. The test sample are tweets posted in the first 5 days of Oct 2018 (with retweets), and in Jan 2018 (without retweets). 
\begin{table}[!htbp]
    \centering
    \hspace{0.5cm}
    \begin{tabular}{ccc}
        Date & Original & Filtered \\ \hline
        2018/10/01 & 4317376 & 5984 \\
        2018/10/02 & 4349129 & 5740 \\
        2018/10/03 & 4417333 & 5415 \\
        2018/10/04 & 4337327 & 5676 \\
        2018/10/05 & 4273031 & 5190 \\
        2018/01 & 134704400 & 37519 \\
    \end{tabular}
    \caption{Tweet counts after flu-related keywords filtering}
    \label{tab:filtering}
\end{table}
\\\\
The result corresponds with experiment conducted by \cite{culotta2010towards}, where the majority of the filtered tweets are irrelevant to keywords. The possible reasons could be that people will use those words even when they are healthy (such as headache), and some words are substring of other words (such as chill-Achill, flu-influence). Another probelm found after this step is that the volume of filtered data is far less than expectation. When retweets are included, nearly 0.129\% data are left. Exclude retweets, the percentage drops to 0.0279\% (although the experiment doesn't operate on a same sample set). In the initial design, geo-tagged tweets (tweets with geographic information) created at certain regions are required. According to \cite{sloan}, geo-tagged tweets are scarce (less than 5\% in their experiment), meaning that the percentage of target data could be much lower (less than 0.001\%). Under such condition, massive data are required to get a reliable estimation. To get 10000 filtered data, at least 100000000 metadata are required. The results indicate that, our dataset may be insufficient for analyzing a single disease (such as flu) when the geographic information is required. Therefore, to get a reliable and convincing dataset, we treat all diseases as a whole, expand the inclusion list to more than 9,000 keywords and keyphrases. The sources are given in \cite{paul2011you}. Table \ref{tab:filtering2} shows the filtering results with new keywords list. More than 30 thousand tweets per day are left, which we believe is sufficient for analysis.
\begin{table}[!htbp]
    \centering
    \hspace{0.5cm}
    \begin{tabular}{ccc}
        Date & total & average(per day) \\ \hline
        2018/01 & 3590773 & 115831 \\
        2018/02 & 1087720 & 40285 \\
        2018/03 & 1108867 & 35769 \\
        2018/04 & 769700 & 33465 \\
        2018/10 & 1035587 & 33406 \\
    \end{tabular}
    \caption{Tweet counts after health-related keywords filtering}
    \label{tab:filtering2}
\end{table}

\section{Supervised classification}
\label{sec:Supervised classification}
Keywords search helps to screen out the majority of unrelated data. However, as proved in the experiment, it can't guarantee the purity of the rest. To alleviate the content-irrelevance probelm found after first round screening, further filtration is required. \cite{elkin2017network} created another exclusion dictionary containing keywords and phrases indicating tweets should not be included in the filtered dataset, such as "sick and tired". Their experimental result shows such method provides roughly 70\% accuracy on their dataset. This method is easy to implement while its accuracy is highly affected by the choice of exclusion words. In section \ref{sec:unsupervised}, we will use unsupervised method to cluster document. Therefore, the outcome of this step decides the final performance of our model. To get a higer accuracy, we decide to use machine learning based classification methods \cite{aramaki2011twitter}. The model we trained in this step is a binary classifier, aiming to detect health-related tweets. Following are our detailed steps:
\begin{enumerate}
    \item Data classification: Labeling strategies are various for different uses. \cite{lampos2010flu} labeled their data with three categories, positive, negative and unknown. We adopt a simple binary labeling strategy, since the ambiguity will decrease the performance of unsupervised clustering used in the next section. Document will be labeled 1 if it indeed related to health and can easily be recognized, 0 otherwise. One exception is our news dataset, since all the documents were posted by official healthy news channels (such as BBChealth, CDChealth), we treat them all as positive samples (63279 tweets in total with 59902 unique words). Apart from that, we randomly labeled 50000 negative samples based on the keywords list. 20\% data are held out as test set while 20\% of training set are used as validation set at each training epoch.
    \item Word embeddings: This step aims to transform documents into vectors and create a look-up table. There are generally two transforming methods: (1) randomly initialise word vector for each word and then continuously update them by learning; (2) pre-train word embeddings on training set. The former one is easy to implement but will need more time on training and it depends more on training set. In our project, we choose to load word embeddings that was pre-trained on large dataset to save training time and increase accuracy. If a word is in the pre-defined dictionary, we will use its vector directly. Otherwise we will initialise its word vector and train it during training. Word2vec\cite{mikolov2013efficient}, Fasttext\cite{joulin2016bag} and Glove \cite{pennington2014glove} are three most common algorithms used to train word embeddings. Although the theories behind them are different, researchers have proven that there is no significant difference among them in practice. While Fasttext and Word2vec train the word vectors through neural networks that are hard to interpret, Glove adopt a more comprehensive method: getting the word representation based on word co-occurrence. For convenience, we use the pre-trained Glove vectors trained on 2 billion tweets with 27 billion tokens and 1.2 million vocabularies. Each word is represented by a 100-dimension vector \cite{pennington2014glove}. An unique index is assigned to each word and its corresponding vector to form up a look-up table. Each document is then transformed to a 2-D matrix based on the table. Note that the number of vocabularies varies among documents. Text can't be reshaped as images, the common solution is pre-define the maximum length of documents and pad missing value with a certain number. In our dataset, most tweets are 5-15 words long, therefore, we define the maximum length of a document is 15. For tweets having less than three words, they are not included in training set. In our implementation, we handle special terms in following rules: For words that either frequently appear in corpus or merely contained by few documents, they are excluded from the document; (2) string `$<pad>'$ is used to pad missing value to extend sentence, and its vector is initialised with zeros; (3) out of vocabulary (OOV) words in new data are represented by string `$<unk>$'. To get meaningful words referring to an extracted topic, we exclude stop words listed by NLTK (The Natural Language Toolkit) \cite{journals/corr/cs-CL-0205028}.
    \item Evaluation: We choose binary cross entropy loss as the loss function while training the model. The formula is: $$\text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)
    \\                     = -x[class] + \log\left(\sum_j \exp(x[j])\right)$$
    Accuracy and F1 score are used to evaluate the performance of the model, their formulas can be found in section \ref{sec:evaluation}.
    \item Modeling and training: The experiment conducted by \cite{serban2019real} shows that deep neural network (DNN) outperform traditional machine learning models such as SVM. With the Glove word representation technique \cite{pennington2014glove}, their model reaches more than 80\% accuracy on their dataset. Based on that, we adopt DNN based models. A basic NLP DNN includes three parts: embedding layer, hidden layers and output layer. Most state-of-the-art models (such as XLNet \cite{yang2019xlnet}) adopt transfer learning to improve their performance. Since our focus is not the improvement on supervised algorithm, and our dataset is relatively small, we assume that neural network with simple structure can meet our expectation. TextCNN \cite{kim2014convolutional} uses multiple different-size kernels to capture features of documents, with a pooling layer and a fully connected layer. After 20 epochs of training, it gets more than 99\% accuracy on training set and more than 98\% accuracy on both validation set and test set. The accuracy hardly improves after 30 epochs. 
    \item Implementation: the major toolkits used for the implementation of our supervised model are Pytorch and Gensim, source code and trained model can be found in \href{https://github.com/NonBee98/FYP}{our GitHub repository} or in the submitted files.
\end{enumerate}
Table \ref{tab:sup1} shows 11 prediction examples of this model. As mentioned before, the accuracy of this model reaches 98.17\% on our test data and 98.2\% on validation set. The table contains both correct and incorrect predictions. Class 1 represents healthcare-related, 0 otherwise. No.1 to No.6 are correct predictions. However, we mainly care about wrong predictions. Document from No.7 to No.11 are typical incorrect prediction samples. In our training set, word ``canada'' appears more in negative samples, therefore, sentences with ``canada'' are more likely be predicted as positive sample. Word ``booze'' is not in training set, therefore No.8 can't be correctly recognized. No.9, No.10 and No.11 are noises, and the model indeed correctly classifies them. Based on this observation, the accuracy of this model can be improved by: (1) training on more data; (2) exclude frequent word; (3) using more advanced methods to handle out-of-vocabulary probelm (such as unknown word inference). 
\begin{table}[!htbp]
    \centering
    \hspace{0.5cm}
    \begin{tabular}{|p{15pt}|p{250pt}|p{60pt}|p{60pt}|}
        \hline
        No. & Text & True class & Prediction \\ \hline
        1 & the move to digital health care is the future but recruitment and training of quality staff needs to happen al & 1 & 1 \\\hline
        2 & catholic school less think kids missed point know literally eve & 0 & 0 \\\hline
        3 & nfl wants players suit over concussions dismissed & 1 & 1\\\hline
        4 & ravaged by typhoon philippines faces threat of serious diseases & 1 & 1 \\\hline
        5 & obama presses leaders to speed ebola response & 1 & 1 \\\hline
        6 & meat seafood prices rising on drought and disease usda & 1 & 1 \\\hline
        7 & wrong initio canada arrest america doesn & 0 & 1 \\\hline
        8 & booze still kills people week & 1 & 0 \\\hline
        9 & top job opportunity ambulance professionals emt nurses physicians around world & 0 & 1 \\\hline
        10 & ad feature & 1 & 0 \\\hline
        11 & course patients first work medical field got stay focused & 0 & 1 \\\hline
    \end{tabular}
    \caption{Sample prediction results of Supervised model}
    \label{tab:sup1}
\end{table}

\section{Unsupervised document clustering and topic generation}
\label{sec:unsupervised}
Supervised model screen out documents that are health-related. To detect unseen diseases and healthcare events, we seek solutions on unsupervised algorithm. According to \cite{allahyari2017brief}, Probability Based Topic Modeling and Machine Learning Based Clustering are two common methods used to group documents. This section will introduce both and provide some experimental results of applying them on our dataset. Then we will explain how our model are build upon them. The evaluation criterion used in this section is topic coherence. Since the generated topics are hard to interpret, scoring each topic with a exact number by human judgments could be difficult and objective, and it requires massive labour.Topic coherence helps to evaluate the quality of generated topics. Experimental results have proven that it has positive correlation with human judgments. In our experiment, we adopt the framework proposed by \cite{roder2015exploring} to calculate the coherence, higer value meaning more coherent. Some libraries such as Gensim provide APIs for using it. For convenience, we uses the service provided \href{https://palmetto.demos.dice-research.org/}{Palmetto}, the Python interface calling the service can be found in our source code. Each generated topic is evaluated by $C_v$, $C_a$, $C_p$ and their sum together. The test dataset is SearchSnippets, it contains 12,295 documents with 8 clusters and 5,547 unique words. For comparison, the number of cluster in our experiment is the same with that in the test set. 

\subsection{Probability based Topic Modeling}
\label{sec:topic modeling}
Topic modeling is a typical tool that is frequently used for discovering abstract topics hidden in documents. It mainly uses techniques of Probability. Early progress includes algorithms such as latent semantic indexing (LSI), Unigram language model and Probabilistic latent semantic analysis (PLSA) \cite{hofmann1999probabilistic, baeza1999modern}. Those algorithms abstract document (d), topic (z) and word (w) from corpus, assume that the generation of a document can be considered as picking a sequence of words from dictionary based on certain probabilistic distribution. Latent Dirichlet Allocation (LDA) \cite{blei2003latent} is a more advanced algorithm under bayesian probability framework. Most of current text modeling algorithms are variants of it. It assumes that: a word is the basic unit of a document, one document could have multiple topics; there are various latent topics that can be characterized by a multinomial distribution over words. The generation of a document can be seen in figure \ref{fig:lda}. It can be explained as following steps: 
\begin{enumerate}
    \item choose the number of words based on poisson distribution and a prior N
    \item choose the topic distribution {$\theta_d$} of this document based on dirichlet distribution with prior {$\alpha$}
    \item for each word {$w_w$} in document {$d$}:
    \begin{enumerate}
        \item choose its topic {$z_w$} based on multinomial distribution with prior {$\theta_d$}
        \item choose a word {$w_w$} based on multinomial probability with prior {$z_w$} and {$\beta$}
    \end{enumerate}
\end{enumerate}
\begin{figure}[!htp]
    \centering
    \begin{tikzpicture}
      [
        observed/.style={minimum size=15pt,circle,draw=blue!50,fill=blue!20},
        unobserved/.style={minimum size=15pt,circle,draw},
        hyper/.style={minimum size=1pt,circle,fill=black},
        post/.style={->,>=stealth',semithick},
      ]
  
      \node (w-j) [observed] at (0,0) {$w_w$};
      \node (z-j) [unobserved] at (-1.5,0) {$z_w$};
      \node (z-prior) [unobserved] at (-3,0) {$\theta_d$};
      \node (z-hyper) [label=above:$\alpha$] at (-4.5,0) {};
      \filldraw [black] (-4.5,0) circle (3pt);
      \node (w-hyper) [label=above:$\beta$] at (-1.5,1.5) {};
      \filldraw [black] (-1.5,1.5) circle (3pt);
      
      \path
      (z-j) edge [post] (w-j)
      
      (z-hyper) edge [post] (z-prior)
      (z-prior) edge [post] (z-j)
  
      (w-hyper) edge [post] (w-j)
      ;
  
      \node [draw,fit=(w-j) (z-prior), inner sep=14pt] (plate-context) {};
      \node [above right] at (plate-context.south west) {$D$};
      \node [draw,fit=(w-j) (z-j), inner sep=10pt] (plate-token) {};
      \node [above right] at (plate-token.south west) {$N$};
  
    \end{tikzpicture}
    \caption{Plate Diagram of LDA.\cite{blei2003latent}}
    \label{fig:lda}
  \end{figure}
By training, the word distribution among topics $P(w|z)$ and the topic distribution among documents $P(z|d)$ can be calculated. Then, each document could be assigned to a topic based on its topic distribution. Each topic can be represented by the first few words that are more likely belong to it. 
\begin{table}[!htbp]
    \centering
    \hspace{0.5cm}
    \begin{tabular}{|p{45pt}|p{210pt}|p{30pt}|p{30pt}|p{30pt}|p{30pt}|p{30pt}|}
        \hline
        Topic ID & Top\_words(10) & $C_v$ & $C_p$ & $C_a$ & $sum_{vpa}$ & $sum_{pa}$\\ \hline
        1 & science edu university research school computer journal culture art program & 0.392 & 0.408 & 0.254 & 1.054 & 0.662\\\hline
        2 & sport game wikipedia com tennis tournament encyclopedia wiki wikipedia\_wiki wikipedia\_encyclopedia & 0.340 & 0.119 & 0.148 & 0.607 & .267 \\\hline
        3 & theory information page ticket gov home home\_page edu library physic & 0.465 & -0.210 & 0.138 & 0.392 & -0.072\\\hline
        4 & health play music information job article com kid news healthy & 0.341 & 0.122 & 0.169 & 0.633 & 0.291\\\hline
        5 & research business software theoretical service data internet network military product & 0.383 & 0.312 & 0.194 & 0.888 & 0.506\\\hline
        6 & match movie yahoo amazon com book directory amazon\_com engine film & 0.345 & 0.054 & 0.171 & 0.570 & 0.225\\\hline
        7 & player news com union market online google poker home car & 0.342 & 0.129 & 0.165 & 0.635 & 0.294 \\\hline
        8 & football news soccer political team rugby party world democracy league & 0.447 & 0.140 & 0.254 & 0.841 & 0.394 \\\hline
        & & 3.054 & 1.075 & 1.493 & 5.622 &2.568\\\hline
       
    \end{tabular}
    \caption{Experimental results of LDA}
    \label{tab:LDA1}
\end{table}
\\\\Table \ref{tab:LDA1} shows the experimental result of it. Parameters of this model is set to default. As can be seen in the table, while evaluating the same topic, $C_v$, $C_p$ and $C_a$ are not always positively correlated with each other, especially for $C_v$ and $C_p$. This is proved by the correlation matrix of them \ref{tab:cormat1}, $C_v$ and $C_p$ are indeed negatively correlated, while $C_a$ has positive correlation with both $C_v$ and $C_p$. Therefore, the sum of them is added and treated as the main criterion for the evaluation. Words in topic 1 contain obvious implication related to education and research that can be easily understood by human, therefore, topic 1 gets the highest sum score. Topic 3 gets the highest $C_v$ score and the lowest $C_p$ and $C_a$ scores. In terms of human judgments, topic 3 is hard to interpret. This means that $C_v$ could be misleading. Hence in the later experiments, $C_v$ is excluded from the evaluation criteria, the final coherence of a model is the sum of $C_p$ and $C_a$. The coherence of the LDA model evaluation on the SearchSnippets dataset is 2.568.
\begin{table}[!htbp]
    \centering
    \hspace{0.5cm}
    \begin{tabular}{|c|c|c|c|}
        \hline
        & $C_v$ & $C_p$ & $C_a$ \\\hline
        $C_v$ & 1 & -0.330 & 0.304 \\\hline
        $C_p$ & -0.330 & 1 & 0.685 \\\hline
        $C_a$ & 0.304 & 0.685 & 1 \\\hline
    \end{tabular}
    \caption{Correlation Matrix of coherence}
    \label{tab:cormat1}
\end{table}

In addition to LDA, we experiment with another more advanced topic model, Biterm Topic Model (BTM) \cite{yan2013biterm, cheng2014btm}, which is designed to modeling short text such as tweets. LDA works well on long document such as news, column reports. However, since it captures the word co-occurrence pattern at document-level, its performance may decrease when running on short document such as tweets due to word sparsity probelm \cite{yan2013biterm}. BTM provides a feasible solution on short text modeling, it learns the co-occurrence pattern of unordered word pairs (a Biterm) at corpus-level and gets a better retrieval result than LDA and PLSA on short text in the experiment conducted by \cite{{yan2013biterm}}. The idea behind BTM is that, one words could contain multiple meaning under different scenarios and topics, while a word-pair is more likely belongs to one topic. 
Table \ref{tab:BTM1} shows the experimental results of BTM applied on SearchSnippets dataset.
\begin{table}[!htbp]
    \centering
    \hspace{0.5cm}
    \begin{tabular}{|p{45pt}|p{210pt}|p{30pt}|p{30pt}|p{30pt}|p{30pt}|}
        \hline
        Topic ID & Top\_words(10) & $C_p$ & $C_a$ & sum\\ \hline
        1 & research edu science information school journal university program gov home & 0.323 & 0.221 & 0.543\\\hline
        2 & movie music com art film video news review photo online & 0.373 & 0.277 & 0.650\\\hline
        3 & wikipedia encyclopedia political system wiki wikipedia\_wiki culture party theory wikipedia\_encyclopedia & 0.064 & 0.167 & 0.231\\\hline
        4 & computer software system web programming memory internet intel com data & 0.249 & 0.223 & 0.472\\\hline
        5 & sport news game football com soccer world match league team & 0.445 & 0.322 & 0.767\\\hline
        6 & business market news stock service trade job finance com information & 0.312 & 0.196 & 0.508\\\hline
        7 & health information cancer disease gov medical drug news calorie healthy & 0.479 & 0.190 & 0.669\\\hline
        8 & amazon com book car amazon\_com engine wheel motor war electrical & -0.034 & 0.190 & 0.156\\\hline
        & & 2.211 & 1.786 & 3.997\\\hline
    \end{tabular}
    \caption{Experimental results of BTM}
    \label{tab:BTM1}
\end{table}
\\Compared with LDA, BTM gets better coherence on both $C_p$ and $C_a$, its sum score is about 56\% higer than that of LDA. With human judgments, we believe this result is convincing, topic coherence is indeed can be used to evaluate the quality of topics and the later comparison will be made based on it.

\subsection{Machine learning based clustering}
\label{sec:Document vector}
Another method for text grouping uses the unsupervised clustering algorithms of machine learning. The idea behind it is mapping original text to a vector space, grouping documents based on clustering algorithm (such as K-means). The key point is how to map documents and assign them with physical meaning. Traditional methods used to represent documents includes bag-of-words and TF-IDF model. They transform words to vectors based on the count of words in the whole corpus and hence has no physical and semantic meaning in a vector space, meaning that directly calculating the distance among to those vectors cannot help to extract the hidden relationships among documents. Word embeddings techniques such as Word2vec \cite{mikolov2013distributed,mikolov2013efficient} introduce some techniques of projecting words into distributed dense word vectors. Word2vec is an unsupervised machine learning algorithm using neural network, and it can learns relationships between words automatically. Each word is represented by a vector with remarkable linear relationships. One example is: vector(“king”) - vector(“man”) + vector(“woman”) =~ vector(“queen”) \cite{journals/corr/cs-CL-0205028}. Such representation is close to human cognition. Inspired by it, documents, topics are possible to be presented in such way. If words, documents and topics share the same space, then their relationships can be calculated easily. For example, if a document vector is more ``close'' to a topic vector, it is more likely belongs to that topic. Doc2vec \cite{le2014distributed} is a variance of Word2vec, it adds one more document vector as the input for each document. Word vectors are shared in corpus while the document is unique for each document. Once the document vectors are trained, clustering algorithms can be applied on them. We experiment this method with a simple K-means cluster. 
After the documents are all grouped, the next step is extracting representive keywords for each topic. Topic model can directly use the calculated probability $P(w|z)$ of each words in each topic, topic $z_i$ can be represented by first few words that are more likely belong to it with higher $P(w|z_i)$. In terms of clustering algorithms, $P(w|z)$ can be represented by the normalised count of a word $\frac{n_w}{\sum_jn_{w_{jz}}}$ in topic z, however, the probelm is that words with higer $P(w|z)$ could be common in all topics, and therefore there could be no significant difference among topics. There are massive algorithms for solving this probelm, including supervised and unsupervised, for the comparison with topic model, we use Term Frequency–Inverse Document Frequency $TFIDF_{i,k}$ in our experiment to rank the importance of words. TF means term frequency while IDF is the abbreviation of Inverse Document Frequency. The formula are listed below, where $n_i,d$ is the number of word i in topic k, $|D_k|$ is the number of instances in topic K, $|{\{j: w_i \in D_k\}}|$ is the number of documents of K that contain word i. In case of there is no instance found in any of the documents, base value 1 is added.
\begin{displaymath}
    \begin{array}{c}
        TF_{i,k} = \frac{n_{i,k}}{\sum_kn_{i,k}}\\
        IDF_{i} = \log\frac{|D_k|}{1 + |\{j: w_i \in D_k\}}| \\
        TFIDF_{i,k} = TF_{i,k} \cdot IDF_{i}\\
    \end{array}
\end{displaymath}
Table \ref{tab:kmeans1} shows the experimental results of K-means based topic modeling. Consider the distinguishability of document vectors and the curse of dimensionality probelm in K-means, the dimension of document vectors is set to 40.
\begin{table}[!htbp]
    \centering
    \hspace{0.5cm}
    \begin{tabular}{|p{45pt}|p{210pt}|p{50pt}|}
        \hline
        Topic ID & Top\_words(10) & $C_v$ score \\ \hline
        1 & computer software product game research web system com business service & 0.39\\ \hline
        2 & research information health science business edu school resource news gov & 0.36 \\ \hline
        3 & gov information research system news business trade political government democracy & 0.35 \\ \hline
        4 & com amazon movie news game research computer system book information & 0.35 \\ \hline
        5 & system computer party culture theory wikipedia football category article game & 0.35 \\ \hline
        6 & news movie com music sport game yahoo world amazon video & 0.37\\\hline
        7 & theory research edu science journal computer physic book system political & 0.38 \\\hline
        8 & theory research edu science news com information game movie sport health online home wikipedia & 0.34 \\\hline
        & & 0.361 \\ \hline
       
    \end{tabular}
    \caption{Experimental results of K-means based clustering}
    \label{tab:kmeans1}
\end{table}
\subsection{Our model}
\label{sec:modeling}
Experimental results show that traditional clustering methods can't help to generate interpretable topics.
Topic model can find representive words of each topic while grouping documents, it's an ideal solution for our purpose. However, it uses the simple bag of words, meaning that its performance could be improved by taking the advantages of word embeddings. Lda2vec \cite{moody2016mixing} combines LDA and word embeddings in a novel way. In traditional CBOW model (one model of Word2vec), the context vector of a word is represented by the sum of its surrounding words' vectors, while in Lda2vec, the context vector of the word is the sum of the word's vector and the document's vector. Each document vector is a mixture of topic vectors, the weight of each topic $z_i$ can be treated as the probability $P(d|z_i)$. In the loss function, an additional LDA loss term is added. Theoretically, Lda2vec can learn topic, word, document vectors simultaneously while calculating the latent topic distribution, however, in practice we find this model requires massive training time, and the generated topics of it are not better than that of LDA in terms of topic coherence. We believe that this model uses too much variables and indeed doesn't use the good properties of word embeddings. \cite{dieng2019topic} assumes that the probability $P(w|z)$ can be got from the inner production of word embedding and vector embedding instead of from a distribution. With pre-trained word embeddings, their model outperform the traditional LDA. Our model takes the advantages of word embeddings and BTM. $\alpha$ is the topic matrix with size L $\times$ K (L: lengdith of embedding, K: number of topics), $\alpha_k$ denotes the vector of $k_{th}$ topic. $\rho$ denotes the word embeddings matrix with size L $\times$ V (V: the number of words). The generation of a corpus can be seen as follows:
\begin{enumerate}
    \item Draw topic proportion $\theta \sim logistc\_normal(\mu, \sigma)$ for the whole corpus. Equation \ref{eq:lognormal} shows the formula of logistc normal.
    \begin{equation}
        \theta=softmax(\psi); \psi \sim Gussian(0,1)
        \label{eq:lognormal}
    \end{equation}
    \item For each biterm b in biterm set B:
    \begin{enumerate}
        \item  draw a topic assignment $z$ $\sim$ $Multinomial(\theta)$
        \item draw two words $w_i$, $w_j$ $\sim$ $softmax(\rho^T\alpha_z)$ 
    \end{enumerate}
\end{enumerate} 
Choosing logistc normal instead of Dirichlet function as the topic distribution benefits the further parameters inference (parameters of distribution model can be directly optimized by back propagation). Step 2(a) is the same with BTM while in 2(b), each word of a biterm is drawn from the production of word matrix and topic embeddings. $softmax(\rho^T\alpha_z)$ is a probability vector denotes the probability $P(w|z)$ of each word in the whole corpus. The marginal probability of each biterm under topic matrix $\alpha$ and the likelihood of the whole corpus are defined as follows:
\begin{equation}
    P(b|\alpha) = \sum_zP(z)P(w_{b_1}|z,\alpha)P(w_{b_s2}|z,\alpha)
    \label{eq:biterm}
\end{equation}
\begin{equation}
    P(B|\alpha) = \prod_bP(b|\alpha)
    \label{eq:corpus}
\end{equation}
In the model, word embeddings $\rho$ is pre-defined parameters while topic embeddings $\alpha$ is unknown and needed to be trained.
To maximize the likelihood is to maximize equation \ref{eq:marginal}:
\begin{equation}
    \begin{aligned}
        \log P(B|a) = \sum_{b}\sum_zP(z)P(w_{b_1}|z,\alpha)P(w_{b_2}|z,\alpha)
    \end{aligned} 
    \label{eq:marginal}
\end{equation}
The conditional probability of a word i in biterm b under topic distribution z and topic matrix $\alpha$ can be calculated by equation \ref{eq:wbi}:
\begin{equation}
    P(w_{bi}|z,\alpha) = \sum_{k}\theta_{b_k}\beta_{k,w_{b_i}}
\label{eq:wbi}
\end{equation}
$\theta_{bk}$ is the topic distribution of biterm b, which is generated by the logistc normal $softmax(Gussian(\mu,\sigma))$  in step 1. $\beta_k,w_{bi}$ is calculated by $softmax(\rho^T\alpha_k)$ in step 2(b). Once the parameters are trained, the topic distribution of documents can be inferd by the assumption proposed by \cite{yan2013biterm}:
\begin{equation}
    P(z|d) = \sum_bP(z|b)P(b|d)
\label{eq:doc1}
\end{equation}
\begin{equation}
    P(z|b) = \frac{P(z)P(w_{b_1}|z)P(w_{b_2}|z)}{\sum_zP(z)P(w_{b_1}|z)P(w_{b_2}|z)}
\label{eq:doc2}
\end{equation}
\begin{equation}
    P(b|d) = \frac{n_d(b)}{\sum_bn_d(b)}
\label{eq:doc3}
\end{equation}
In equation \ref{eq:doc1} assumes $P(z|b)$ can be represented by the expectation of the topic proportions of biterms. In equation \ref{eq:doc3}, $n_d(b)$ represents the number of biterm b in document d.
\\\\
The next question is how to infer the parameters. Equation \ref{eq:marginal} can't be directly optimized since the integral of marginal probability $P(b|\alpha)$ is intractable (EM algorithm cannot be used). In LDA and BTM, Gibbs sampling is used to infer parameters. However, such method requires Dirichlet distribution. Since our model draw words based on the production of $\rho^T\alpha$, same method can't be applied. To infer the parameters, we use Variational inference\cite{wainwright2008graphical} instead. Variational inference helps to create a simpler distribution that could best approach the original intractable distribution. By adopting variational inference, the original statistical inference probelm becomes an optimization problem. Our goal now is to find a distribution $Q(z; \phi)$ that has the least difference with original marginal distribution of biterm in equation \ref{eq:biterm}. Here z is the variational distribution and $\phi$ is the variational parameters of q.
In step 1, we assume that the original topic distribution of a biterm is drawn from a Gussian distribution, therefore, q is defined as a Gussian, whose mean and variance is calculated by its parameters. The next step is learning the variational parameters. \cite{kingma2013auto} proposes a method that could automatically learn the variational parameter based on given input (in our model is the biterm set B), the process can be seen as below:
\begin{equation}
    Q(z;\phi;b) \leftarrow Gussian(\mu_b,\sigma_b)
\label{eq:var1}
\end{equation}
\begin{equation}
    \mu_b = f(\phi_{\mu}, b)
\label{eq:var2}
\end{equation}
\begin{equation}
    \sigma_b = g(\phi_{\sigma}, b)
\label{eq:var3}
\end{equation}
Recall that a biterm is a unordered words pair, and in our model, each word is represented by its embedding. Therefore, in our model, each biterm is defined as the sum of its corresponding words' embeddings (equation \ref{eq:biterm2}). As stated before, the pre-trained word embeddings has semantic and physic meaning (e.g. vec(``phone'') + vec(``apple'') = vec(``iphone'')), such representation uses this good property. Each biterm is assigned with semantic meaning, and this information will be used in parameter inference.
\begin{equation}
    b = w_{b_1} + w_{b_2}
\label{eq:biterm2}
\end{equation}
Function $f(\phi_{\mu}, b)$ and $g(\phi_{\sigma}, b)$ are two neural networks with no bias (there is no special requirements of the networks, they could be simple NNs with one linear layer or with more complex structure). They are called ``encoder'' by \cite{kingma2013auto} since they receive a data (or a batch of data) and output the parameters of its corresponding distribution model (in term of Gussian, the outputs are mean and variance). $\phi_{\mu}$ and $\phi_{\sigma}$ are the parameters of this two neural networks respectively, and they will be updated by back propagation.\\\\
The final step is defining the loss function. Recall that $P(z)$ is the real topic distribution and $Q(z;b;\phi)$ is the approximation distribution, $\phi$ is the parameter set of q, we want to find $\phi$ that can minimize their ``difference'' (it's obviously that a simple distribution of P(z) will benefit the inference of $\phi$, which is one reason why we use logistc normal in step 1). The ``difference'' between $P(z)$ and $Q(z;b;\phi)$ is commonly evaluated by the  the Kullback-Leibler (KL) divergence \cite{yang2017understanding}. The KL divergence for variational inference is defined as follows: 
\begin{equation}
    \begin{aligned}
        KL(Q(z;b,\phi)||P(z)) & = \sum_zQ(z;b,\phi)\log\frac{Q(z;b,\phi)}{P(z)}\\
    \end{aligned} 
    \label{eq:KL}
\end{equation}
KL divergence can be treated as information loss, it is always $\geq$ 0. When KL divergence equals 0, $q(z;b,\phi)$ and $P(z)$ is the same distribution. In our model, we want the KL divergence is as small as possible. In addition, we want to maximize the likelihood of $P(B,\theta|\alpha)$, and this can be done by maximize equation \ref{eq:loss2}:
\begin{equation}
    \sum_b\log P(b_{w_1}|z_b,\alpha)+\log P(b_{w_2}|z_b,\alpha)
    \label{eq:loss2}
\end{equation}
Therefore, the final loss function is defined as equation \ref{eq:loss}:
\begin{equation}
    \begin{aligned}
        loss(model, B, \rho) & = \sum_b\left[KL(Q(z;b,\phi)||P(z))-\log P(b_{w_1}|z_b,\alpha)\right] 
    \end{aligned}    
    \label{eq:loss}
\end{equation}
By using the automatic derivation technique, parameters of our model can be updated by back propagation. One probelm is that random operation cannot be derived(backpropagated) since it doesn't have gradient. For example, given a variable x,y with arbitrary values, and a variable $z = Gussian(x,y)$, $loss(x,y) = (x-y)^2$, back propagation cannot be applied to the loss function. To solve this probelm, make the loss function derivable, we use the reparameterization trick \cite{kingma2013auto}. In terms of the same example, z now equals to $x + Gussian(0,1) * y$. The distribution of z is the same, while the latter one can be backpropagated. To use the trick, we adopt the logistc normal rather than Dirichlet in step 1.
\begin{algorithm}
    \caption{Topic distribution}
    \LinesNumbered 
    \KwIn{$D$: documents set $D=\{d_1,d_2\dots d_n\}$\newline
    $\rho$: pre-trained word embeddings
    }
    \KwOut{$\alpha$: learned topic embeddings\newline
    $\phi$: learned parameter set of Q}
    Pre-process D, build vocabulary and encode documents in D to indexes \\
    Initialise a biterm set $B = \{\}$ \\
    \For{$d_i \in D$}{
        biterms $\leftarrow$ $extract_biterm(d_i)$\\
        $B \leftarrow B \union biterms$
    }
    \For{$i = 1 to max\_iteration$}
    {
        \For{topic k in K}
        {
            Compute $\beta_k$ = $softmax(\rho)$
        } 
    }
\end{algorithm}