\section{Ethical Issues}
Although this project got previous ethical approve from campus, it doesn't mean that this project doesn't contain any potential ethical issues. Following are possible ethical issues related to this project.
\subsection{Privacy Issues}
\label{privacy}
As mentioned above, we will collect social media data from at least one platform in this project. At this stage, a critical ethical issue is that whether our collection will infringe upon others' right. Here ``others'' are data providers, social media platforms and the users. According to the permutation and combination, this issue can be divided into following six categories:
\begin{enumerate}
    \item This project and data providers
    \item This project and social media platforms
    \item This project and users
    \item Data provider and social media platforms
    \item Data provider and users
    \item Social media platforms and users
\end{enumerate}
So far, our data was downloaded from an open source dataset, and in its privacy policy, using the data for research is allowed, there is no confliction between our project and the data provider. However, whether the data providers are authorized by the social media platforms and whether the users allow the platform to share their data are not known (although according to \cite{burkell2014facebook}, once the data is published on social without requirement for confidentiality, it become public). If neither the platforms nor the providers are authorized by the users, should only they be blamed or our project will also be implicated in the privacy issue (the project didn't collect the data directly from users)?
\subsection{Trust, Safety and Reliability Issues}
In this project, there is actually an basic assumption: the social media data can be analysed for healthcare surveillance. Although \cite{andreu2015big,denecke2009valuable,lamb2013separating} showed that social media data can be used to predict the outbreak of disease (means that the congruity between the data and the real records is not a coincidence), but so far, no algorithms can guarantee that their prediction is absolutely correct. If the incorrect results are trusted by the public, they may trigger some trust and reliability issues (such as if the algorithm incorrectly predict an outbreak of disease and the prediction is known by the public, it could results in undesired events such as social panic).
\subsection{Vulnerable Groups Issues}
Suppose that the accuracy of the algorithm is accurate enough. If the prediction is used in positive ways (such as used by hospitals to prepare medical resources for upcoming disease), it can be beneficial. However, if the prediction is used in negative, some people might become vulnerable. (one possible condition is that some people might be isolated because they are from areas where disease transmission could take place).
\section{Evaluation and Analysis}
\subsection{Privacy Analysis}
According to \cite{felt2008privacy}, although most social media data are public and are not protected, there are still some private information that can't be collected. 
In section \ref{privacy}, six possible sources of ethical issues are listed. The first one comes from social media platforms and their users. Since this project solely analyse Twitter dataset currently, the section mainly focus on it. According to Twitter's privacy policy \cite{twitter_pri}, Twitter can be both public and non-public. While using Twitter, some personal information will be received, such as the type of device users are using and their IP address. Apart from that, users can freely decided whether to share more information such as e-mail address, phone number, a public profile, etc. \\
For this project, three types of information will be collected from users: (1) The creation time of a tweet; (2) The geographic information showing where the tweet is posted; (3) The content of the tweets. In the section 1.2 ``Public Information'' of the policy (accessed on Dec 2, 2019), it states that the following activities on Twitter are public: (1) profile information; (2) the language and time zone users' are using; (3) the creation time of users' account; (4) Tweets and its creation time, the version of Twitter etc.
In the section 2.1, Twitter states that users can share or hide their location information such as their current precise position or places where they have previously used Twitter, which means that all the location data showing in Twitter are allowed by users (in principle). Based on these statement, users are responsible for the content they post on the Twitter. However,if the platform doesn't obey the policy, collects and uses users' hiden data, discloses those data intentionally or by accident (such as due to technical problem), it should be punished. Taking Facebook as an example, it was fined 5 billion dollars for information leak in July 2019. But note that, sometimes ethical issues could happen even the policy is followed strictly, for example, sharing location or other metadata can actually affect other people than the uploader himself \cite{smith2012big}. Platforms need to take such issues in consideration.\\\\
The second possible confliction comes from users and third parties (data providers and this project). Firstly, suppose that all the data collected by third parties are authorized by platforms and platforms are authorized by users. In this case, third parties don't need authorization from users, and if they follow the privacy policy, there won't be confliction \cite{tankard2012big}. But what if the platforms provide unauthorized data to third parties, and third parties use those data in their service. Stated by \cite{felt2008privacy}, host social networking sites are responsible for protecting the user data that has been entrusted to them. Application developers can access to data that would not otherwise be available to them through
the APIs provided by platforms. This supports that the networking sites should be punished under this condition. The most complex question in this scenario is that whether the third parties should also be punished? John Stuart Mill's Utilitarianism theory \cite{mill2016utilitarianism} states that, wether an act or an intention is morely accessed is depends merely on the its consequence, if a act can maximize the happiness or follow most rules, it is moral. In terms of this project, the aim is to prevent illness and benefit public health, it therefore can increase the happiness of all. But at meanwhile, using unauthorized data breaks the privacy law, which is regarded as immoral based on this theory. In contrast to Utilitarianism, Deontological Theory judge the morality of choices by the will of them insetad of the result, some acts are forbidden if its starting point is morally bad, even if the result is good \cite{alexander2007deontological}. Here the using of unauthorized information is immoral through Kantian Reasoning. Assume the act is ok, everyone can use such data without authorization, then there is no unauthorized information, the rule makes no sense and therefore the third parties' keep using those data is immoral. \\\\
The thrid possible confliction comes from data providers and this project. In the private policy of Archive site \cite{archive_pri}, it states that data is free to use but is granted for scholarship and research purposes only. As mentioned above, this project will solely use the data for research analysis, and won't violate the privacy policy.

\subsection{Reliability Analysis}
Unlike the real records collected from hospitals and laboratories, prediction can be incorrect. Following are causes that may lead to prediction failure in this project:
\begin{enumerate}
    \item Insufficient data: since this algorithm relies solely on social media data, it can be highly affected by the data volume. One supporting example is the experiment done by \cite{elkin2017network}, they found that during festivals, the number of Tweets decreased significantly, and the prediction accuracy also lowered at that time. 
    \item Noise data: the algorithm uses NLP techniques, it will train a model that can tell whether a Tweet is relevant to healthcare or not. But if there are noises in training dataset, the prediction will be affected. In addition, even if all the train data are correctly labeled, and the model can reach a high accuracy, it still can not guarantee the prediction is what will really happen. For example, if a person is worrying about getting ill, but there are actually no disease outbreak (even human cannot tell), they algorithm may make an wrong prediction. Apart from that, noises can come from unexpected ways, such as a celebrity got ill, and there were massive message talking about that\cite{schmidt2012trending}. 
    \item Algorithm itself: so far, there is no model that can simulate the reality with 100\% accuracy. Model can reach a high score on certain dataset but may perform badly on others. Especially for new data, the performance of algorithm can not be expected \cite{andreu2015big}.
\end{enumerate}
Assume the algorithm will adopted by hospitals to prepare for preventing against upcoming disease. If the result is false positive (the prediction is positive but wrong), hospitals may waste medical resources, even worse, if the result is revealed, it may causes social panic. If such case happen, who will take the responsibility? According to Birsch's moral responsibility \cite{birsch2004moral}, people reaching the three criteria should bear the responsibility: (1) the action of the person caused the harm bring some other bad consequences; (2) the person inflict the harm intentionally or the harm was caused by his recklessness or carelessness; (3) the person know the possible consequences in advance and still choose to neglect it. If we allow the hospitals to use this algorithm without telling them the possible failure, we will be punished. However, if the disadvantages are told in advance, organizations that used the algorithm should take the consequences.
\subsection{Vulnerable Groups Analysis}
This project could harm to some groups if it is used inappropriately. Here the possible vulnerable groups are: (1)citizens living in places that are forecasted a disease spreading, they may become target and at risk of being victims of hate-motivated behaviour; (2) citizens who don't know the forecast, they may lose the equal right when receiving medical resources (people know the prediction in advance have more chances to take actions, such as moving to other city, receive medical treatment early, etc.). \\
ACM Code of Ethics and Professional Conduct states that we ensure our product won't harm others \cite{ACM_C}. Therefore, solutions are required to prevent such condition. One important concern is that who can access to the system and query for the forecast. ``I have nothing to hide'' argument states that hiding information can be regarded as concealment, it can be benefit criminals rather than common citizens, and therefore it is more harmful to the whole society\cite{solove2007ven}. According to this statement, the result should be transparent, public should be able to access to the system easily. Centers for Disease Control and Prevention (CDC) adoptes this strategy and update its flu records weekly to public \cite{cdc:fluView}. However, as mentioned before, in a worst case, the prediction can result in social panic, some people even could be isolated. On the contrary, if the system can solely used by one person, it might lose its advantages in disease prevention, since few institutions can take action against the coming illness outbreak. The ideal situation is that the information will be kept solely by healthcare institutions such as official hospitals, and won't be leaked. But in reality, this can hardly be reached, how to balance the social emotion and the prediction usage is the key to solve the problem.
 