{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T11:15:52.719932Z",
     "start_time": "2020-03-24T11:15:50.879701Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import gensim\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "from utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T11:16:38.655255Z",
     "start_time": "2020-03-24T11:16:38.652441Z"
    }
   },
   "outputs": [],
   "source": [
    "import smart_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T12:31:06.673650Z",
     "start_time": "2020-03-24T12:31:06.669186Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line, min_len=3)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T12:31:57.538357Z",
     "start_time": "2020-03-24T12:31:57.487818Z"
    }
   },
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(\"/Volumes/White/training/bbchealth.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T12:32:53.420331Z",
     "start_time": "2020-03-24T12:32:53.349862Z"
    }
   },
   "outputs": [],
   "source": [
    "test_corpus = list(read_corpus(\"/Volumes/White/training/cbchealth.txt\", tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T12:33:26.861850Z",
     "start_time": "2020-03-24T12:33:26.858150Z"
    }
   },
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T12:33:32.843909Z",
     "start_time": "2020-03-24T12:33:31.715380Z"
    }
   },
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T12:33:46.903189Z",
     "start_time": "2020-03-24T12:33:38.466205Z"
    }
   },
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T12:45:56.105321Z",
     "start_time": "2020-03-24T12:45:15.559440Z"
    }
   },
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T12:46:05.395290Z",
     "start_time": "2020-03-24T12:46:05.390312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 299, 1: 101, 2: 79, 3: 73, 6: 52, 5: 49, 4: 47, 7: 44, 8: 43, 10: 42, 9: 40, 11: 37, 17: 37, 14: 37, 13: 33, 27: 33, 21: 32, 19: 31, 20: 30, 12: 30, 26: 29, 47: 29, 18: 28, 24: 28, 15: 27, 29: 26, 23: 26, 40: 24, 41: 24, 22: 24, 32: 23, 48: 23, 25: 22, 33: 22, 43: 22, 79: 21, 16: 21, 28: 21, 54: 20, 42: 20, 30: 19, 31: 18, 78: 18, 35: 18, 38: 18, 55: 17, 34: 17, 36: 17, 60: 17, 50: 17, 84: 17, 94: 17, 68: 17, 45: 17, 90: 16, 88: 16, 70: 16, 100: 16, 66: 16, 52: 15, 37: 15, 106: 15, 73: 15, 138: 15, 61: 15, 46: 15, 82: 14, 58: 14, 101: 14, 51: 14, 80: 14, 71: 14, 111: 14, 39: 14, 91: 14, 115: 14, 112: 14, 74: 13, 107: 13, 76: 13, 93: 13, 75: 13, 110: 13, 127: 13, 67: 13, 179: 13, 81: 13, 92: 13, 63: 13, 118: 13, 59: 13, 65: 13, 44: 12, 89: 12, 166: 12, 133: 12, 139: 12, 124: 12, 121: 12, 120: 12, 102: 12, 56: 12, 53: 12, 159: 11, 131: 11, 155: 11, 69: 11, 174: 11, 119: 11, 98: 11, 62: 11, 104: 11, 128: 11, 77: 11, 87: 11, 156: 11, 147: 10, 64: 10, 132: 10, 130: 10, 99: 10, 141: 10, 135: 10, 142: 10, 200: 10, 114: 10, 199: 10, 49: 10, 125: 10, 137: 10, 96: 9, 180: 9, 187: 9, 109: 9, 144: 9, 209: 9, 83: 9, 95: 9, 164: 9, 108: 9, 136: 9, 190: 9, 85: 9, 260: 9, 116: 9, 165: 9, 57: 9, 158: 8, 134: 8, 72: 8, 140: 8, 97: 8, 160: 8, 117: 8, 175: 8, 203: 8, 225: 8, 245: 8, 197: 8, 161: 8, 113: 7, 153: 7, 168: 7, 208: 7, 204: 7, 167: 7, 151: 7, 223: 7, 269: 7, 152: 7, 122: 7, 184: 7, 126: 7, 183: 7, 234: 6, 103: 6, 198: 6, 170: 6, 254: 6, 129: 6, 238: 6, 177: 6, 86: 6, 218: 6, 291: 6, 150: 6, 202: 6, 189: 6, 172: 6, 123: 6, 163: 6, 217: 5, 146: 5, 325: 5, 195: 5, 292: 5, 148: 5, 143: 5, 196: 5, 173: 5, 181: 5, 244: 5, 263: 5, 145: 5, 149: 5, 290: 5, 105: 5, 193: 5, 169: 5, 201: 5, 154: 5, 206: 5, 305: 5, 211: 5, 280: 5, 219: 4, 205: 4, 186: 4, 185: 4, 210: 4, 232: 4, 212: 4, 304: 4, 191: 4, 229: 4, 237: 4, 178: 4, 258: 4, 272: 4, 240: 4, 282: 4, 215: 4, 276: 4, 271: 4, 268: 3, 251: 3, 262: 3, 228: 3, 221: 3, 295: 3, 410: 3, 453: 3, 264: 3, 176: 3, 300: 3, 302: 3, 306: 3, 250: 3, 355: 3, 265: 3, 318: 3, 322: 3, 241: 3, 216: 3, 231: 3, 278: 3, 253: 3, 277: 3, 351: 3, 246: 3, 273: 3, 314: 3, 157: 3, 257: 3, 289: 3, 316: 3, 171: 3, 227: 3, 323: 3, 274: 3, 162: 3, 207: 3, 249: 3, 267: 3, 281: 3, 194: 3, 224: 3, 236: 3, 308: 3, 401: 2, 356: 2, 261: 2, 266: 2, 407: 2, 286: 2, 442: 2, 332: 2, 333: 2, 242: 2, 270: 2, 489: 2, 330: 2, 182: 2, 363: 2, 288: 2, 309: 2, 369: 2, 432: 2, 372: 2, 373: 2, 424: 2, 390: 2, 259: 2, 342: 2, 256: 2, 319: 2, 284: 2, 324: 2, 239: 2, 287: 2, 339: 2, 296: 2, 360: 2, 233: 2, 416: 2, 275: 2, 503: 2, 371: 2, 255: 2, 331: 2, 220: 2, 226: 2, 328: 2, 378: 2, 327: 2, 335: 2, 213: 2, 299: 2, 469: 2, 447: 2, 414: 2, 230: 2, 584: 2, 188: 2, 477: 2, 449: 2, 222: 2, 365: 2, 394: 2, 301: 2, 298: 2, 336: 2, 367: 2, 3192: 2, 370: 1, 353: 1, 192: 1, 1034: 1, 374: 1, 313: 1, 481: 1, 321: 1, 3582: 1, 3514: 1, 1024: 1, 2183: 1, 320: 1, 761: 1, 859: 1, 303: 1, 3569: 1, 607: 1, 731: 1, 252: 1, 482: 1, 514: 1, 533: 1, 375: 1, 496: 1, 3799: 1, 682: 1, 411: 1, 2976: 1, 3776: 1, 214: 1, 3579: 1, 3801: 1, 531: 1, 3322: 1, 434: 1, 3235: 1, 599: 1, 502: 1, 283: 1, 408: 1, 557: 1, 598: 1, 329: 1, 555: 1, 383: 1, 435: 1, 1781: 1, 545: 1, 1046: 1, 368: 1, 536: 1, 2506: 1, 513: 1, 409: 1, 748: 1, 3331: 1, 1068: 1, 506: 1, 459: 1, 294: 1, 2746: 1, 690: 1, 431: 1, 317: 1, 427: 1, 562: 1, 404: 1, 3733: 1, 463: 1, 343: 1, 764: 1, 589: 1, 1203: 1, 566: 1, 312: 1, 412: 1, 534: 1, 881: 1, 243: 1, 293: 1, 971: 1, 2349: 1, 326: 1, 608: 1, 499: 1, 3490: 1, 3485: 1, 3730: 1, 666: 1, 393: 1, 3230: 1, 1750: 1, 461: 1, 615: 1, 2374: 1, 349: 1, 554: 1, 392: 1, 474: 1, 708: 1, 247: 1, 490: 1, 3435: 1, 454: 1, 528: 1, 3311: 1, 3120: 1, 311: 1, 310: 1, 625: 1, 358: 1, 402: 1, 426: 1, 399: 1, 509: 1, 315: 1, 437: 1, 483: 1, 3124: 1, 346: 1, 565: 1, 471: 1, 415: 1, 376: 1, 3768: 1, 457: 1, 3502: 1, 941: 1, 428: 1, 478: 1, 3506: 1, 2409: 1, 834: 1, 3830: 1, 385: 1, 235: 1, 3780: 1, 297: 1, 397: 1, 341: 1, 556: 1, 660: 1, 559: 1, 831: 1, 441: 1, 530: 1, 611: 1, 403: 1, 386: 1, 1036: 1, 3646: 1, 907: 1, 479: 1, 307: 1, 3691: 1, 782: 1, 467: 1, 743: 1, 605: 1, 364: 1, 875: 1, 1071: 1, 352: 1, 626: 1, 775: 1, 529: 1, 3744: 1, 248: 1})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T13:14:53.606242Z",
     "start_time": "2020-03-24T13:14:53.600984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (3928): «diff manslaughter inquiry call»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (1913, 0.9228657484054565): «hospital deaths inquiry announced»\n",
      "\n",
      "SECOND-MOST (3245, 0.9043088555335999): «trust apologises over dna failings»\n",
      "\n",
      "MEDIAN (135, 0.8058278560638428): «testosterone boost could cut deaths»\n",
      "\n",
      "LEAST (1849, -0.890856146812439): «audio uk mum spent to have girl»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T13:17:45.860039Z",
     "start_time": "2020-03-24T13:17:45.854844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (18): «public back tax rises to fund nhs»\n",
      "\n",
      "Similar Document (532, 0.7212491631507874): «video could volunteers help crisis»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "import random\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T13:23:31.968287Z",
     "start_time": "2020-03-24T13:23:31.944525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (3053): «research to help you pick and choose best way to sneeze»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (99, 0.8253598213195801): «video what can make you happy»\n",
      "\n",
      "MEDIAN (1166, 0.0705452412366867): «video robbie sings during wife labour»\n",
      "\n",
      "LEAST (1929, -0.7627078294754028): «ebola deaths up by in west africa»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T05:10:33.174958Z",
     "start_time": "2020-03-25T05:10:33.170647Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2VecVocab at 0x12a0f0390>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T06:40:37.001431Z",
     "start_time": "2020-03-25T06:40:36.996757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3741"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
