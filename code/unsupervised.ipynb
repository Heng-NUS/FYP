{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T08:59:02.695400Z",
     "start_time": "2020-04-20T08:59:02.690890Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import smart_open\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases, LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from palmettopy.palmetto import Palmetto\n",
    "palmetto = Palmetto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T08:59:03.042531Z",
     "start_time": "2020-04-20T08:59:03.022472Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "labels = []\n",
    "\n",
    "with open('./cluster/SearchSnippets.txt','r') as d_f:\n",
    "    for line in d_f:\n",
    "        if line != '\\n':\n",
    "            docs.append(line)\n",
    "    \n",
    "with open('./cluster/SearchSnippets_label.txt', 'r') as l_f:\n",
    "    for line in l_f:\n",
    "        if line != '\\n':\n",
    "            labels.append(int(line))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T08:59:07.893393Z",
     "start_time": "2020-04-20T08:59:04.601074Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T08:59:29.754933Z",
     "start_time": "2020-04-20T08:59:29.514781Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T08:59:31.419067Z",
     "start_time": "2020-04-20T08:59:31.400122Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('doc_info.txt', 'w', encoding='utf-8') as f:\n",
    "    for doc in docs:\n",
    "        f.write(' '.join(doc) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T08:59:33.155393Z",
     "start_time": "2020-04-20T08:59:33.011913Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T08:59:34.469170Z",
     "start_time": "2020-04-20T08:59:34.465260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 3912\n",
      "Number of documents: 12295\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T09:10:41.359675Z",
     "start_time": "2020-04-20T09:09:57.278402Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set training parameters.\n",
    "num_topics = 8\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 100\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T09:10:42.051755Z",
     "start_time": "2020-04-20T09:10:41.990229Z"
    }
   },
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(corpus, topn=10) #, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T09:10:42.683298Z",
     "start_time": "2020-04-20T09:10:42.677285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "research science edu journal theory page information paper university theoretical \n",
      "art culture system engine history home information fitness music page \n",
      "sport news football com match hockey team club rugby volleyball \n",
      "health business information gov news union social job service disease \n",
      "wikipedia tennis encyclopedia wiki wikipedia_wiki wikipedia_encyclopedia political basketball yahoo wimbledon \n",
      "game com tournament online school amazon book university espn ticket \n",
      "soccer player computer world system score cup software internet republic \n",
      "movie film imdb award equipment space resource gym electrical forum \n"
     ]
    }
   ],
   "source": [
    "topics = []\n",
    "for i in range(len(top_topics)):\n",
    "    a = [x[1] for x in top_topics[i][0]]\n",
    "    topics.append(a)\n",
    "    for x in a:\n",
    "        print(x, end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T07:59:58.695254Z",
     "start_time": "2020-04-20T07:59:58.677749Z"
    }
   },
   "outputs": [],
   "source": [
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(tokens, [i]) for i, tokens in enumerate(docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T08:00:50.563573Z",
     "start_time": "2020-04-20T08:00:50.559018Z"
    }
   },
   "outputs": [],
   "source": [
    "model2 = gensim.models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T08:00:06.193967Z",
     "start_time": "2020-04-20T08:00:03.386921Z"
    }
   },
   "outputs": [],
   "source": [
    "model2.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T08:00:37.342734Z",
     "start_time": "2020-04-20T08:00:06.487928Z"
    }
   },
   "outputs": [],
   "source": [
    "model2.train(train_corpus, total_examples=model2.corpus_count, epochs=model2.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T08:00:37.668369Z",
     "start_time": "2020-04-20T08:00:37.648446Z"
    }
   },
   "outputs": [],
   "source": [
    "docvecs = [model2.docvecs[i] for i in range(len(docs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T08:39:48.553958Z",
     "start_time": "2020-04-20T08:39:48.550832Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T08:40:01.690226Z",
     "start_time": "2020-04-20T08:40:00.732051Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans.fit(docvecs)\n",
    "predicted = kmeans.predict(docvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T08:40:14.290760Z",
     "start_time": "2020-04-20T08:40:14.281562Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster_dict = {}\n",
    "for i,label in enumerate(predicted):\n",
    "    if label in cluster_dict:\n",
    "        cluster_dict[label].append(i)\n",
    "    else:\n",
    "        cluster_dict[label] = [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T08:05:19.713459Z",
     "start_time": "2020-04-20T08:05:19.706062Z"
    }
   },
   "outputs": [],
   "source": [
    "def top_words(cluster_dict, dictionary, word_num=10):\n",
    "    all_dict = []\n",
    "    topics = []\n",
    "    for x in range(len(cluster_dict)):\n",
    "        group0 = [dictionary.doc2idx(docs[i]) for i in cluster_dict[x]]\n",
    "        topic_word_count = {}\n",
    "        for doc in group0:\n",
    "            for x in doc:\n",
    "                if x in topic_word_count:\n",
    "                    topic_word_count[x] += 1\n",
    "                else:\n",
    "                    topic_word_count[x] = 1\n",
    "        all_dict.append(topic_word_count)\n",
    "        \n",
    "    for topic_word_count in all_dict:\n",
    "        totoal_count = sum(topic_word_count.values())\n",
    "        tf_idf = {}\n",
    "        for w in topic_word_count.keys():\n",
    "            dfs = 0\n",
    "            for dic in all_dict:\n",
    "                if w in dic:\n",
    "                    dfs += 1\n",
    "            try:\n",
    "                tf = topic_word_count[w] / totoal_count\n",
    "                idf = np.log(len(cluster_dict) / (dfs))\n",
    "                tf_idf[w] = tf * idf\n",
    "            except KeyError:\n",
    "                continue\n",
    "        a = sorted(tf_idf.items(), key=lambda x:x[1], reverse=True)[:word_num]\n",
    "        topic0 = [dictionary[i[0]] for i in a]\n",
    "        topics.append(topic0)\n",
    "        print(' '.join(topic0))\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T08:40:26.843345Z",
     "start_time": "2020-04-20T08:40:26.695182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commodity tax medicare fda insurance tariff fund_budget agency union venture\n",
      "medicare cba union minnesota referee senator economic_development public_health dentist agency\n",
      "cisco mozilla wireless_access client_server zdnet microprocessor ibm sourceforge mspx cache\n",
      "bbc allposters commodity sportsline lyric chron boxing cbs bull forbes\n",
      "britannica union britannica_article descartes communism encyclopaedia_britannica westminster socialism meaning fluid\n",
      "sewing bull sewing_machine chicago_bull speed_test stock_quote tiger commodity client_server ticket\n",
      "stanford_edu mit lecture ocw optic einstein aristotle wolfram maa reasoning\n",
      "lyric girl youtube movie_episode episode piano soundtrack olympic tiger favorite\n"
     ]
    }
   ],
   "source": [
    "topics = top_words(cluster_dict, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T10:17:29.896797Z",
     "start_time": "2020-04-20T10:15:14.210364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 & research edu science journal theory school university information computer program & 0.424 & 0.256 & 0.680\\\\\\hline\n",
      "2 & movie com music art amazon culture book film video news & 0.316 & 0.248 & 0.564\\\\\\hline\n",
      "3 & computer software web system memory programming internet com intel device & 0.249 & 0.217 & 0.466\\\\\\hline\n",
      "4 & wikipedia political encyclopedia system party wiki wikipedia_wiki democracy wikipedia_encyclopedia government & 0.095 & 0.181 & 0.276\\\\\\hline\n",
      "5 & sport news game football com soccer world match league ticket & 0.360 & 0.254 & 0.614\\\\\\hline\n",
      "6 & business market news service stock trade job information home finance & 0.294 & 0.174 & 0.468\\\\\\hline\n",
      "7 & health information gov cancer news research disease medical drug national & 0.426 & 0.227 & 0.653\\\\\\hline\n",
      "8 & car engine calorie wheel motor electrical income tax model automatic & 0.103 & 0.232 & 0.335\\\\\\hline\n",
      "& & 2.266 & 1.790 & 4.056\\\\\\hline\n"
     ]
    }
   ],
   "source": [
    "cps = []\n",
    "cas = []\n",
    "sums = []\n",
    "for i,topic in enumerate(a):\n",
    "    cp = palmetto.get_coherence(topic,coherence_type=\"cp\")\n",
    "    ca = palmetto.get_coherence(topic, coherence_type=\"ca\")\n",
    "    cps.append(cp)\n",
    "    cas.append(ca)\n",
    "    allsum = cp+ca\n",
    "    sums.append(allsum)\n",
    "    topic = \" \".join(topic)\n",
    "    print(r\"{} & {} & {:.3f} & {:.3f} & {:.3f}\\\\\\hline\".format(i+1, topic, cp, ca, allsum))\n",
    "sum_cp = sum(cps)\n",
    "sum_ca = sum(cas)\n",
    "print(r\"& & {:.3f} & {:.3f} & {:.3f}\\\\\\hline\".format(sum_cp, sum_ca, sum_cp+sum_ca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T09:46:02.197798Z",
     "start_time": "2020-04-20T09:46:02.193202Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-24-fdfdea0d1edf>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-fdfdea0d1edf>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    a = \"research edu science journal school theory university information computer program\u001b[0m\n\u001b[0m                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "research edu science journal theory school university information computer program\n",
    "movie com music art amazon culture book film video news\n",
    "computer software web system memory programming internet com intel device\n",
    "wikipedia political encyclopedia system party wiki wikipedia_wiki democracy wikipedia_encyclopedia government\n",
    "sport news game football com soccer world match league ticket\n",
    "business market news service stock trade job information home finance\n",
    "health information gov cancer news research disease medical drug national\n",
    "car engine calorie wheel motor electrical income tax model automatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T10:14:07.683935Z",
     "start_time": "2020-04-20T10:14:07.681346Z"
    }
   },
   "outputs": [],
   "source": [
    "a = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T10:15:01.198308Z",
     "start_time": "2020-04-20T10:15:01.195302Z"
    }
   },
   "outputs": [],
   "source": [
    "a.append(\"car engine calorie wheel motor electrical income tax model automatic\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:07:14.489244Z",
     "start_time": "2020-04-22T12:07:13.294554Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T02:12:46.977702Z",
     "start_time": "2020-04-23T02:12:46.972571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:21:56.390359Z",
     "start_time": "2020-04-22T12:21:56.335790Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zero_() missing 1 required positional arguments: \"input\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-d4d1f9235970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: zero_() missing 1 required positional arguments: \"input\""
     ]
    }
   ],
   "source": [
    "torch.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:07:51.721642Z",
     "start_time": "2020-04-22T12:07:51.716416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0907, 0.3435])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
