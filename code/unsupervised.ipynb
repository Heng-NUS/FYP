{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T03:31:49.915618Z",
     "start_time": "2020-04-15T03:31:49.907326Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import smart_open\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases, LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from palmettopy.palmetto import Palmetto\n",
    "palmetto = Palmetto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T09:38:36.391815Z",
     "start_time": "2020-04-09T09:38:36.369671Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "labels = []\n",
    "\n",
    "with open('./cluster/SearchSnippets.txt','r') as d_f:\n",
    "    for line in d_f:\n",
    "        if line != '\\n':\n",
    "            docs.append(line)\n",
    "    \n",
    "with open('./cluster/SearchSnippets_label.txt', 'r') as l_f:\n",
    "    for line in l_f:\n",
    "        if line != '\\n':\n",
    "            labels.append(int(line))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T09:38:40.405870Z",
     "start_time": "2020-04-09T09:38:38.756039Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T09:39:15.311665Z",
     "start_time": "2020-04-09T09:39:15.074493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T07:50:15.963901Z",
     "start_time": "2020-04-09T07:50:15.945721Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('doc_info.txt', 'w', encoding='utf-8') as f:\n",
    "    for doc in docs:\n",
    "        f.write(' '.join(doc) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:56:33.127894Z",
     "start_time": "2020-04-09T06:56:32.980689Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:56:34.405775Z",
     "start_time": "2020-04-09T06:56:34.401445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 4294\n",
      "Number of documents: 12295\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T07:06:29.511885Z",
     "start_time": "2020-04-09T07:05:47.966121Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set training parameters.\n",
    "num_topics = 8\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T07:06:29.776957Z",
     "start_time": "2020-04-09T07:06:29.709399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -6.1084.\n",
      "[([(0.027648749, 'science'),\n",
      "   (0.025220705, 'edu'),\n",
      "   (0.02249564, 'university'),\n",
      "   (0.020326843, 'research'),\n",
      "   (0.016762562, 'school'),\n",
      "   (0.015941538, 'computer'),\n",
      "   (0.015501143, 'journal'),\n",
      "   (0.0139552485, 'culture'),\n",
      "   (0.013372158, 'art'),\n",
      "   (0.0110983625, 'program')],\n",
      "  -3.125355490676686),\n",
      " ([(0.06994312, 'sport'),\n",
      "   (0.062722966, 'game'),\n",
      "   (0.040915854, 'wikipedia'),\n",
      "   (0.028579878, 'com'),\n",
      "   (0.023078991, 'tennis'),\n",
      "   (0.021796566, 'tournament'),\n",
      "   (0.021678928, 'encyclopedia'),\n",
      "   (0.020678993, 'wiki'),\n",
      "   (0.020116704, 'wikipedia_wiki'),\n",
      "   (0.019209446, 'wikipedia_encyclopedia')],\n",
      "  -4.278761410348151),\n",
      " ([(0.021935984, 'theory'),\n",
      "   (0.019099936, 'information'),\n",
      "   (0.018759515, 'page'),\n",
      "   (0.018291483, 'ticket'),\n",
      "   (0.017691605, 'gov'),\n",
      "   (0.01688735, 'home'),\n",
      "   (0.014780757, 'home_page'),\n",
      "   (0.013993921, 'edu'),\n",
      "   (0.013960634, 'library'),\n",
      "   (0.01340576, 'physic')],\n",
      "  -5.389017553090616),\n",
      " ([(0.033694528, 'health'),\n",
      "   (0.02253539, 'play'),\n",
      "   (0.021575728, 'music'),\n",
      "   (0.011979411, 'information'),\n",
      "   (0.011423554, 'job'),\n",
      "   (0.010591587, 'article'),\n",
      "   (0.008725276, 'com'),\n",
      "   (0.008681563, 'kid'),\n",
      "   (0.008338219, 'news'),\n",
      "   (0.0074021127, 'healthy')],\n",
      "  -5.468287813483363),\n",
      " ([(0.026022298, 'research'),\n",
      "   (0.022841046, 'business'),\n",
      "   (0.016061535, 'software'),\n",
      "   (0.014403209, 'theoretical'),\n",
      "   (0.013588105, 'service'),\n",
      "   (0.0121271135, 'data'),\n",
      "   (0.011636396, 'internet'),\n",
      "   (0.0113134915, 'network'),\n",
      "   (0.01119085, 'military'),\n",
      "   (0.01108701, 'product')],\n",
      "  -6.401343599423677),\n",
      " ([(0.045144424, 'match'),\n",
      "   (0.042928245, 'movie'),\n",
      "   (0.037741728, 'yahoo'),\n",
      "   (0.033065278, 'amazon'),\n",
      "   (0.03113648, 'com'),\n",
      "   (0.02877404, 'book'),\n",
      "   (0.02056612, 'directory'),\n",
      "   (0.020452065, 'amazon_com'),\n",
      "   (0.01651328, 'engine'),\n",
      "   (0.013507463, 'film')],\n",
      "  -7.303986838677668),\n",
      " ([(0.048788078, 'player'),\n",
      "   (0.025693456, 'news'),\n",
      "   (0.024577802, 'com'),\n",
      "   (0.012581069, 'union'),\n",
      "   (0.011333807, 'market'),\n",
      "   (0.011132791, 'online'),\n",
      "   (0.010956356, 'google'),\n",
      "   (0.010926086, 'poker'),\n",
      "   (0.010116236, 'home'),\n",
      "   (0.008941463, 'car')],\n",
      "  -7.988064240302342),\n",
      " ([(0.04756557, 'football'),\n",
      "   (0.032412454, 'news'),\n",
      "   (0.030712815, 'soccer'),\n",
      "   (0.021892557, 'political'),\n",
      "   (0.02118624, 'team'),\n",
      "   (0.019886917, 'rugby'),\n",
      "   (0.015376617, 'party'),\n",
      "   (0.014958341, 'world'),\n",
      "   (0.01442654, 'democracy'),\n",
      "   (0.014096112, 'league')],\n",
      "  -8.912663569280502)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus, topn=10) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T07:06:29.981865Z",
     "start_time": "2020-04-09T07:06:29.977560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "science edu university research school computer journal culture art program \n",
      "sport game wikipedia com tennis tournament encyclopedia wiki wikipedia_wiki wikipedia_encyclopedia \n",
      "theory information page ticket gov home home_page edu library physic \n",
      "health play music information job article com kid news healthy \n",
      "research business software theoretical service data internet network military product \n",
      "match movie yahoo amazon com book directory amazon_com engine film \n",
      "player news com union market online google poker home car \n",
      "football news soccer political team rugby party world democracy league \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(top_topics)):\n",
    "    a = [x[1] for x in top_topics[i][0]]\n",
    "    for x in a:\n",
    "        print(x, end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T09:39:24.897941Z",
     "start_time": "2020-04-09T09:39:24.875905Z"
    }
   },
   "outputs": [],
   "source": [
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(tokens, [i]) for i, tokens in enumerate(docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T11:01:31.234342Z",
     "start_time": "2020-04-09T11:01:31.230371Z"
    }
   },
   "outputs": [],
   "source": [
    "model2 = gensim.models.doc2vec.Doc2Vec(vector_size=20, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T11:01:53.433893Z",
     "start_time": "2020-04-09T11:01:50.537791Z"
    }
   },
   "outputs": [],
   "source": [
    "model2.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T11:02:41.532873Z",
     "start_time": "2020-04-09T11:02:12.477554Z"
    }
   },
   "outputs": [],
   "source": [
    "model2.train(train_corpus, total_examples=model2.corpus_count, epochs=model2.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T11:03:00.835232Z",
     "start_time": "2020-04-09T11:03:00.811435Z"
    }
   },
   "outputs": [],
   "source": [
    "docvecs = [model2.docvecs[i] for i in range(len(docs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T11:03:12.796692Z",
     "start_time": "2020-04-09T11:03:12.794105Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T11:03:25.819022Z",
     "start_time": "2020-04-09T11:03:24.837138Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans.fit(docvecs)\n",
    "predicted = kmeans.predict(docvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T11:03:37.856329Z",
     "start_time": "2020-04-09T11:03:37.849239Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster_dict = {}\n",
    "for i,label in enumerate(predicted):\n",
    "    if label in cluster_dict:\n",
    "        cluster_dict[label].append(i)\n",
    "    else:\n",
    "        cluster_dict[label] = [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T10:14:39.335959Z",
     "start_time": "2020-04-09T10:14:39.328823Z"
    }
   },
   "outputs": [],
   "source": [
    "def top_words(cluster_dict, dictionaryï¼Œword_num=10):\n",
    "    all_dict = []\n",
    "    topics = []\n",
    "    for x in range(len(cluster_dict)):\n",
    "        group0 = [dictionary.doc2idx(docs[i]) for i in cluster_dict[x]]\n",
    "        topic_word_count = {}\n",
    "        for doc in group0:\n",
    "            for x in doc:\n",
    "                if x in topic_word_count:\n",
    "                    topic_word_count[x] += 1\n",
    "                else:\n",
    "                    topic_word_count[x] = 1\n",
    "        all_dict.append(topic_word_count)\n",
    "        \n",
    "    for topic_word_count in all_dict:\n",
    "        totoal_count = sum(topic_word_count.values())\n",
    "        tf_idf = {}\n",
    "        for w in topic_word_count.keys():\n",
    "            dfs = 0\n",
    "            for dic in all_dict:\n",
    "                if w in dic:\n",
    "                    dfs += 1\n",
    "            try:\n",
    "                tf = topic_word_count[w] / totoal_count\n",
    "                idf = np.log(len(cluster_dict) / (dfs))\n",
    "                tf_idf[w] = tf * idf\n",
    "            except KeyError:\n",
    "                continue\n",
    "        a = sorted(tf_idf.items(), key=lambda x:x[1], reverse=True)[:word_num]\n",
    "        topic0 = [dictionary[i[0]] for i in a]\n",
    "        topics.append(topic0)\n",
    "        print(' '.join(topic0))\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T11:03:58.454712Z",
     "start_time": "2020-04-09T11:03:49.897613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia news information com health research movie system business political\n",
      "amazon com game movie book system research computer news amazon_com\n",
      "news business information research health com movie yahoo school science\n",
      "research gov information health news cancer national system economic government\n",
      "computer com software system product intel information web research news\n",
      "research science edu theory journal computer physic course university information\n",
      "news com music movie sport game football world video online\n",
      "wikipedia encyclopedia wikipedia_encyclopedia wiki wikipedia_wiki system political democracy culture article\n"
     ]
    }
   ],
   "source": [
    "top_words(cluster_dict, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T04:17:32.975649Z",
     "start_time": "2020-04-11T04:17:25.029085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34044923952367706"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palmetto.get_coherence(\"wikipedia news information com health research movie system business political\".split(),coherence_type=\"cv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T03:32:12.062553Z",
     "start_time": "2020-04-15T03:32:11.546118Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T03:32:37.512121Z",
     "start_time": "2020-04-15T03:32:37.505817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T03:36:08.996475Z",
     "start_time": "2020-04-15T03:36:08.993696Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.zeros((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T03:36:52.926417Z",
     "start_time": "2020-04-15T03:36:52.923611Z"
    }
   },
   "outputs": [],
   "source": [
    "a[1][1] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T03:36:54.465054Z",
     "start_time": "2020-04-15T03:36:54.460414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
